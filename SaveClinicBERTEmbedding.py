# -*- coding: utf-8 -*-
"""SaveClinicBERTEmbedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mhbRpBN1Twjbu8ZnHLJWCI5DjLoL1cmH
"""

from transformers import AutoTokenizer, AutoModel
import pandas as pd
import torch
import re
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

def load_annotation_data():
    """Load the annotated dataset with row IDs from Google Drive"""
    annotation_path = "/content/drive/MyDrive/MIMIC-SBDH.csv"
    columns = ['row_id', 'sdoh_community_present', 'sdoh_community_absent',
               'sdoh_education', 'sdoh_economics', 'sdoh_environment',
               'behavior_alcohol', 'behavior_tobacco', 'behavior_drug']

    annotated_data = pd.read_csv(annotation_path, usecols=columns)
    return annotated_data

def load_mimic_data():
    """Load the original MIMIC-III discharge summaries from Google Drive"""
    mimic_path = "/content/drive/MyDrive/NOTEEVENTS.csv"
    mimic_data = pd.read_csv(
        mimic_path,
        usecols=['ROW_ID', 'TEXT', 'CATEGORY']
    )

    # Filter for discharge summaries only
    return mimic_data[mimic_data['CATEGORY'] == 'Discharge summary']

def extract_social_history(text):
    """Extract social history section from discharge summary"""
    if pd.isna(text):
        return ""
    pattern = r"social history:\s*(.*?)(?=\n[a-z\s]+:|\Z)"
    match = re.search(pattern, text.lower(), re.DOTALL)
    return match.group(1).strip() if match else ""

def prepare_dataset(annotated_data, mimic_data):
    """Merge and prepare the dataset"""
    annotated_data = annotated_data.rename(columns={'row_id':'ROW_ID'})
    merged = pd.merge(
        annotated_data,
        mimic_data[['ROW_ID', 'TEXT']],
        on='ROW_ID'
    )
    merged['social_history'] = merged['TEXT'].apply(extract_social_history)
    return merged[merged['social_history'] != ""]

#%% Calculate the embedding using pre-trained bio clinical BERT and save the result
data = prepare_dataset(load_annotation_data(), load_mimic_data())
print(f"Number of samples: {len(data)}")

texts = data['social_history']
texts_list = texts.tolist()
X = np.empty((0, 768))
tokenizer = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
model = AutoModel.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
for i in range(len(texts_list)):
    print("processing ",i)
    inputs = tokenizer(texts_list[i], return_tensors="pt", padding="max_length", truncation=True,max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()  # Take [CLS] token
        X = np.append(X, embeddings, axis=0)

np.save('/content/drive/MyDrive/bert-embed.npy', X)
