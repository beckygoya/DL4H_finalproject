# -*- coding: utf-8 -*-
"""Bio-Clinical BERT_0504.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18WY7i_mfVfnWvYk2mR-H5-3ruCE9ilvx
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import re
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score
from collections import Counter
from sklearn.utils import resample
import warnings
warnings.filterwarnings('ignore')

# Load the MIMIC-SBDH dataset
def load_annotation_data():
    annotation_path = "/content/drive/MyDrive/MIMIC-SBDH.csv"
    columns = ['row_id', 'sdoh_community_present', 'sdoh_community_absent',
               'sdoh_education', 'sdoh_economics', 'sdoh_environment',
               'behavior_alcohol', 'behavior_tobacco', 'behavior_drug']
    return pd.read_csv(annotation_path, usecols=columns)

# Load the original MIMIC-III dataset and the discharge summaries
def load_mimic_data():
    mimic_path = "/content/drive/MyDrive/NOTEEVENTS.csv"
    mimic_data = pd.read_csv(mimic_path,usecols=['ROW_ID', 'TEXT', 'CATEGORY'])
    return mimic_data[mimic_data['CATEGORY'] == 'Discharge summary']

# Extract social history section from discharge summary
def extract_social_history(text):
    if pd.isna(text):
        return ""
    pattern = r"social history:\s*(.*?)(?=\n[a-z\s]+:|\Z)"
    match = re.search(pattern, text.lower(), re.DOTALL)
    return match.group(1).strip() if match else ""

# Merge and prepare the dataset
def prepare_dataset(annotated_data, mimic_data):
    annotated_data = annotated_data.rename(columns={'row_id':'ROW_ID'})
    merged = pd.merge(
        annotated_data,
        mimic_data[['ROW_ID', 'TEXT']],
        on='ROW_ID'
    )
    merged['social_history'] = merged['TEXT'].apply(extract_social_history)
    return merged[merged['social_history'] != ""]
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load the Bio-ClinicalBERT model and tokenizer
MODEL_NAME = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Custom Dataset class
class SBDHDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            text.lower(),
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Oversampling function to handle class imbalance
def oversample_data(X, y):
    class_counts = Counter(y)
    max_count = max(class_counts.values())
    X_resampled = []
    y_resampled = []
    for class_label in class_counts:
        X_class = X[y == class_label]
        y_class = y[y == class_label]
        if len(X_class) < max_count:
            X_class_resampled, y_class_resampled = resample(
                X_class, y_class,
                n_samples=max_count,
                random_state=42
            )
        else:
            X_class_resampled, y_class_resampled = X_class, y_class

        X_resampled.extend(X_class_resampled)
        y_resampled.extend(y_class_resampled)

    return np.array(X_resampled), np.array(y_resampled)

# Training function for a single SBDH
def train_sbdh_model(X, y, column_name, num_labels, n_splits=3):
    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    f1_scores = []

    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):
        print(f"\nTraining fold {fold + 1}/{n_splits} for {column_name}")
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        X_train_resampled, y_train_resampled = oversample_data(X_train, y_train)
        train_dataset = SBDHDataset(X_train_resampled, y_train_resampled, tokenizer)
        val_dataset = SBDHDataset(X_val, y_val, tokenizer)

        model = AutoModelForSequenceClassification.from_pretrained(
            MODEL_NAME,
            num_labels=num_labels
        ).to(device)

        # Set up training arguments
        training_args = TrainingArguments(
            output_dir=f'./results_{column_name}_fold_{fold}',
            num_train_epochs=10,  # Reduced epochs from 50 in the paper to 10
            warmup_steps=500,
            lr_scheduler_type="linear",
            per_device_train_batch_size=32,
            per_device_eval_batch_size=32,
            learning_rate=5e-5,
            weight_decay=0.01,
            eval_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            fp16=True,
            gradient_accumulation_steps=2,
            logging_steps=100,
            save_total_limit=1,
            seed=42,
            report_to="none"
        )

        # Custom compute metrics function
        def compute_metrics(eval_pred):
            predictions, labels = eval_pred
            predictions = np.argmax(predictions, axis=1)
            return {'f1': f1_score(labels, predictions, average='macro')}

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            compute_metrics=compute_metrics,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
        )

        trainer.train()
        eval_results = trainer.evaluate()
        f1_scores.append(eval_results['eval_f1'])
        del model
        del trainer
        torch.cuda.empty_cache()

    mean_f1 = np.mean(f1_scores)
    std_f1 = np.std(f1_scores)
    print(f"\n{column_name} - Mean F1: {mean_f1:.4f} ± {std_f1:.4f}")
    return mean_f1, std_f1

# Main execution function
def main():
    print("Loading annotation data...")
    annotated_data = load_annotation_data()
    print(f"Loaded {len(annotated_data)} annotations")

    print("Loading MIMIC-III data...")
    mimic_data = load_mimic_data()
    print(f"Loaded {len(mimic_data)} discharge summaries")

    print("Preparing dataset...")
    df = prepare_dataset(annotated_data, mimic_data)
    print(f"Final dataset size: {len(df)}")

    X = df['social_history'].values

    sbdh_columns = {
        'sdoh_community_present': 2, 'sdoh_community_absent': 2,
        'sdoh_education': 2, 'sdoh_economics': 3,
        'sdoh_environment': 3, 'behavior_alcohol': 5,
        'behavior_tobacco': 5, 'behavior_drug': 5
    }

    results = {}

    # Train model for each SBDH
    for column, num_labels in sbdh_columns.items():
        print(f"\n{'='*50}")
        print(f"Training for {column}")
        print(f"{'='*50}")

        y = df[column].values
        unique_values = np.unique(y)
        print(f"Unique values in {column}: {unique_values}")

        actual_num_labels = len(unique_values)
        if actual_num_labels > num_labels:
            print(f"Warning: Found {actual_num_labels} unique values, expected {num_labels}")
            num_labels = actual_num_labels

        mean_f1, std_f1 = train_sbdh_model(X, y, column, num_labels)
        results[column] = {'mean_f1': mean_f1, 'std_f1': std_f1}

    print("\n\nFinal Results:")
    print("="*50)
    for column, metrics in results.items():
        print(f"{column}: {metrics['mean_f1']:.4f} ± {metrics['std_f1']:.4f}")

    # Save results to file
    results_df = pd.DataFrame.from_dict(results, orient='index')
    results_df.to_csv('/content/drive/MyDrive/mimic_sbdh_results.csv')
    print("\nResults saved to /content/drive/MyDrive/mimic_sbdh_results.csv")

if __name__ == "__main__":
    # Install required packages
    !pip install transformers datasets scikit-learn pandas torch numpy

    main()